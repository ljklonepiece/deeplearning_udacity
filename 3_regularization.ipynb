{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 24.001616\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 10.8%\n",
      "Minibatch loss at step 500: 2.594838\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 1.861284\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 1.450855\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 1.036859\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.746310\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.817337\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding the best beta value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4,-2,0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: regul}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIiIiIiIiIiIiIiIiIiIiIkXw/+8g29vDz4bwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9c570c650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add 1-layer of relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_units = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_units]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_units, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 633.237549\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 32.0%\n",
      "Minibatch loss at step 500: 197.129822\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1000: 117.527763\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1500: 69.296387\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 41.348621\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2500: 25.259254\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3000: 15.380573\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_units = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_units]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_units, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 659.548401\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 30.9%\n",
      "Minibatch loss at step 500: 191.545197\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 1000: 116.163574\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 1500: 70.447914\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 2000: 42.723495\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.1%\n",
      "Minibatch loss at step 2500: 25.909887\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 3000: 15.713236\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Test accuracy: 71.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 3\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_units = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_units]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_units, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 754.752197\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 41.0%\n",
      "Minibatch loss at step 500: 190.895859\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 1000: 115.790474\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 1500: 70.226028\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 2000: 42.589592\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.8%\n",
      "Minibatch loss at step 2500: 25.829050\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 3000: 15.664398\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.5%\n",
      "Test accuracy: 78.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 3\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 253\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes1], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    \n",
    "    \n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 3.292729\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 40.3%\n",
      "Minibatch loss at step 500: 0.965746\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1000: 0.862125\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1500: 0.788374\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2000: 0.548730\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2500: 0.557195\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3000: 0.515178\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3500: 0.548760\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4000: 0.498945\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 4500: 0.481838\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 5000: 0.410481\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5500: 0.384543\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6000: 0.417740\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 6500: 0.379542\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.445047\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 7500: 0.356956\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8000: 0.537663\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.306826\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9000: 0.372271\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9500: 0.436690\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10000: 0.320840\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10500: 0.420017\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11000: 0.366057\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11500: 0.380570\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 12000: 0.390159\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 12500: 0.516617\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 13000: 0.370946\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 13500: 0.456381\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 14000: 0.365644\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 14500: 0.555068\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 15000: 0.351239\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 15500: 0.459429\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 16000: 0.355662\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 16500: 0.357304\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 17000: 0.480012\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 17500: 0.311757\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000: 0.289670\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps  = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step%500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
